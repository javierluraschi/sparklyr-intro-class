library(sparklyr)
spark_install()                             # Install Apache Spark
sc <- spark_connect(master="local")         # Connect to local instance
library(dplyr)                              # Data Manipulation Grammar
mtcars_tbl <- copy_to(sc, mtcars)           # Copy mtcars into Spark
mtcars_tbl %>% summarize(n = n())           # Count records
mtcars_tbl %>% ml_linear_regression(        # Perform linear regression
response = "mpg",                         # Response vector
features = c("wt", "cyl"))                # Features for the model fit
library(DBI)                                # R Database Interface
dbGetQuery(sc, "SELECT * FROM mtcars")      # Run SQL query in Spark
invoke(spark_context(sc), "version")        # Run sc.version in Scala
knit_with_parameters('~/RStudio/rstudio-other/spark-summit-2017/spark-summit-2017.Rmd')
?haven::read_sav
normalizePath("~/Spark")
options(rsparkling.sparklingwater.version = "1.6.8")
library(rsparkling)
library(sparklyr)
library(dplyr)
library(h2o)
sc <- spark_connect(master = "local")
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
library(sparklyr)                       # Load sparklyr
spark_install()                         # Install Apache Spark
sc <- spark_connect(master = "local")   # Connect to local instance
library(dplyr)                          # Data Manipulation Grammar
mtcars_tbl <- copy_to(sc, mtcars)       # Copy mtcars into Spark
count(mtcars_tbl)                       # Count records
ml_linear_regression(mtcars_tbl,        # Perform linear regression
response = "mpg",                     # Response vector
features = c("wt", "cyl"))            # Features for the model fit
library(DBI)                            # R Database Interface
dbGetQuery(sc, "SELECT * FROM mtcars")  # Run SQL query in Spark
invoke(spark_context(sc), "version")    # Run sc.version in Scala
library(sparkinstall)
spark_install(version = "1.6.2")    # Install Spark from R
delay <- flights_tbl %>%
group_by(tailnum) %>%
summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
filter(count > 20, dist < 2000, !is.na(delay)) %>%
collect
# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
geom_point(aes(size = count), alpha = 1/2) +
geom_smooth() +
scale_size_area(max_size = 2)
delay <- flights_tbl %>%
group_by(tailnum) %>%
summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
filter(count > 20, dist < 2000, !is.na(delay)) %>%
collect
# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
geom_point(aes(size = count), alpha = 1/2) +
geom_smooth() +
scale_size_area(max_size = 2)
library(DBI)
dbGetQuery(sc, "SELECT * FROM flights LIMIT 100")
# transform our data set, and then partition into 'training', 'test'
partitions <- mtcars_tbl %>%
filter(hp >= 100) %>%
mutate(cyl8 = cyl == 8) %>%
sdf_partition(training = 0.5, test = 0.5, seed = 1099)
# fit a linear model to the training dataset
partitions$training %>%
ml_linear_regression(response = "mpg", features = c("wt", "cyl"))
options(rsparkling.sparklingwater.version = "1.6.8")
library(rsparkling)
library(sparklyr)
library(dplyr)
library(h2o)
sc <- spark_connect(master = "local")
mtcars_tbl <- copy_to(sc, mtcars)
mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)
h2o.glm(x = c("wt", "cyl"),
y = "mpg",
training_frame = mtcars_h2o,
lambda_search = TRUE)
mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)
mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)
mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)
mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)
mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)
mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)
mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)
h2o.glm(x = c("wt", "cyl"),
y = "mpg",
training_frame = mtcars_h2o,
lambda_search = TRUE)
spark_disconnect(sc)
library(sparklygraphs)
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local", version = "2.1.0")
spark_disconnect(sc)
library(sparklygraphs)
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local", version = "2.1.0")
highschool_tbl <- copy_to(sc, ggraph::highschool, "highschool")
# create a table with unique vertices using dplyr
vertices_tbl <- sdf_bind_rows(
highschool_tbl %>% distinct(from) %>% transmute(id = from),
highschool_tbl %>% distinct(to) %>% transmute(id = to)
)
# create a table with <source, destination> edges
edges_tbl <- highschool_tbl %>% transmute(src = from, dst = to)
# calculate PageRank over the highschool dataset
gf_graphframe(vertices_tbl, edges_tbl) %>%
gf_pagerank(reset_prob = 0.15, max_iter = 10L, source_id = "1")
spark_apply(highschool_tbl, function(x) {
x + rgamma(1,2)
})
spark_apply(highschool_tbl, function(x) {
x
})
spark_apply(highschool_tbl, function(x) {
x + rgamma(1,2)
})
spark_disconnect(sc)
options(rsparkling.sparklingwater.version = "1.6.8")
library(rsparkling)
library(sparklyr)
library(dplyr)
library(h2o)
sc <- spark_connect(master = "local")
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
library(sparklyr)                       # Load sparklyr
spark_install()                         # Install Apache Spark
sc <- spark_connect(master = "local")   # Connect to local instance
library(dplyr)                          # Data Manipulation Grammar
mtcars_tbl <- copy_to(sc, mtcars)       # Copy mtcars into Spark
count(mtcars_tbl)                       # Count records
?mtcars_tbl
?mtcars
ml_linear_regression(mtcars_tbl,        # Perform linear regression
response = "mpg",                     # Response vector
features = c("wt", "cyl"))            # Features for the model fit
library(DBI)                            # R Database Interface
dbGetQuery(sc, "SELECT * FROM mtcars")  # Run SQL query in Spark
library(DBI)                            # R Database Interface
dbGetQuery(sc, "SELECT * FROM mtcars LIMIT 100")  # Run SQL query in Spark
library(DBI)                            # R Database Interface
dbGetQuery(sc, "SELECT * mtcars")  # Run SQL query in Spark
library(DBI)                            # R Database Interface
dbGetQuery(sc, "SELECT * FROM mtcars")  # Run SQL query in Spark
invoke(spark_context(sc), "version")    # Run sc.version in Scala
spark_installed_versions()
spark_installed_versions()
spark_available_versions()
flights_tbl
?nycflights
?nycflights13::flights
delay <- flights_tbl %>%
group_by(tailnum) %>%
summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
filter(count > 20, dist < 2000, !is.na(delay)) %>%
collect
# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
geom_point(aes(size = count), alpha = 1/2) +
geom_smooth() +
scale_size_area(max_size = 2)
library(DBI)
dbGetQuery(sc, "SELECT * FROM flights LIMIT 100")
library(DBI)
dbGetQuery(sc, "SELECT count(*) FROM flights GROUP BY tailnum")
library(DBI)
dbGetQuery(sc, "SELECT tailnum, count(*) FROM flights GROUP BY tailnum")
library(DBI)
dbGetQuery(sc, "SELECT tailnum, n = count(*) FROM flights GROUP BY tailnum ORDER BY n")
library(DBI)
dbGetQuery(sc, "SELECT tailnum, total = count(*) FROM flights GROUP BY tailnum ORDER BY total")
library(DBI)
dbGetQuery(sc, "SELECT * FROM (SELECT tailnum, total = count(*) FROM flights GROUP BY tailnum) ORDER BY total")
library(DBI)
dbGetQuery(sc, "SELECT * FROM flights LIMIT 100")
partitions <- mtcars_tbl %>%
filter(hp >= 100) %>%
mutate(cyl8 = cyl == 8) %>%
sdf_partition(training = 0.5, test = 0.5, seed = 1099)
# fit a linear model to the training dataset
partitions$training %>%
ml_linear_regression(response = "mpg", features = c("wt", "cyl"))
partitions$training %>%
ml_logistic_regression(response = "mpg", features = c("wt", "cyl"))
partitions$training %>%
ml_generalized_linear_regression(response = "mpg", features = c("wt", "cyl"))
ml_kmeans(iris_tbl, features = c("wt", "cyl"))
partitions$training %>%
ml_kmeans(features = c("wt", "cyl"))
partitions$training %>%
ml_kmeans(centers = 3, features = c("wt", "cyl"))
mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)
h2o.glm(x = c("wt", "cyl"),
y = "mpg",
training_frame = mtcars_h2o,
lambda_search = TRUE)
h2o_flow(sc)
h2o_flow(sc, strict_version_check = FALSE)
spark_disconnect(sc)
library(sparklygraphs)
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local", version = "2.1.0")
highschool_tbl <- copy_to(sc, ggraph::highschool, "highschool")
# create a table with unique vertices using dplyr
vertices_tbl <- sdf_bind_rows(
highschool_tbl %>% distinct(from) %>% transmute(id = from),
highschool_tbl %>% distinct(to) %>% transmute(id = to)
)
# create a table with <source, destination> edges
edges_tbl <- highschool_tbl %>% transmute(src = from, dst = to)
# calculate PageRank over the highschool dataset
gf_graphframe(vertices_tbl, edges_tbl) %>%
gf_pagerank(reset_prob = 0.15, max_iter = 10L, source_id = "1")
spark_apply(highschool_tbl, function(x) {
x + rgamma(1,2)
})
spark_apply(highschool_tbl, function(x) {
x
})
spark_apply(highschool_tbl, function(x) {
x + rgamma(1,2)
})
spark_apply(highschool_tbl, function(x) {
paste(x)
}, names = "concat")
spark_apply(highschool_tbl, function(x) {
paste(x, collapse = "-")
}, names = "concat")
iris
paste(iris)
spark_apply(highschool_tbl, function(x) {
lapply(x, function(r) paste(r))
}, names = "concat")
spark_apply(highschool_tbl, function(x) {
lapply(x, function(r) paste(r))
})
spark_apply(highschool_tbl, function(x) {
lapply(x, function(r) paste(r, collapse = "-"))
}, names = "concat")
spark_apply(highschool_tbl, function(x) {
dapply(x, function(r) paste(r, collapse = "-"))
}, names = "concat")
dapply(iris, function(r) paste(r, collapse = "-"))
apply(iris, function(r) paste(r, collapse = "-"))
spark_apply(highschool_tbl, function(x) {
x
})
spark_apply(highschool_tbl, function(x) {
x$year - 1900
})
spark_apply(highschool_tbl, function(x) {
x$year - 1900
}, names = "year")
spark_apply(highschool_tbl, function(x) {
paste(x$year - 1900, "s")
}, names = "year")
spark_apply(highschool_tbl, function(x) {
paste(x$year - 1900, "s", sep = "'")
}, names = "year")
spark_apply(highschool_tbl, function(x) {
x$name <- paste(x$year - 1900, "s", sep = "'")
x
})
spark_apply(highschool_tbl, function(x) {
x$name <- paste(x$year - 1900, "s", sep = "'")
x
}, names = c(names, "name"))
spark_apply(highschool_tbl, function(x) {
x$name <- paste(x$year - 1900, "s", sep = "'")
x
}, names = c(colnames(x), "name"))
spark_apply(highschool_tbl, function(x) {
x$name <- paste(x$year - 1900, "s", sep = "'")
x
}, names = c(colnames(highschool_tbl), "name"))
spark_disconnect(sc)
typeof(1)
typeof(1L)
spark_disconnect(sc)
options(rsparkling.sparklingwater.version = "1.6.8")
library(rsparkling)
library(sparklyr)
library(dplyr)
library(h2o)
sc <- spark_connect(master = "local")
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
library(sparklyr)                       # Load sparklyr
spark_install()                         # Install Apache Spark
sc <- spark_connect(master = "local")   # Connect to local instance
library(dplyr)                          # Data Manipulation Grammar
mtcars_tbl <- copy_to(sc, mtcars)       # Copy mtcars into Spark
count(mtcars_tbl)                       # Count records
ml_linear_regression(mtcars_tbl,        # Perform linear regression
response = "mpg",                     # Response vector
features = c("wt", "cyl"))            # Features for the model fit
library(DBI)                            # R Database Interface
dbGetQuery(sc, "SELECT * FROM mtcars")  # Run SQL query in Spark
invoke(spark_context(sc), "version")    # Run sc.version in Scala
compile_package_jars()                  # Compile Scala code
delay <- flights_tbl %>%
group_by(tailnum) %>%
summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
filter(count > 20, dist < 2000, !is.na(delay)) %>%
collect
# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
geom_point(aes(size = count), alpha = 1/2) +
geom_smooth() +
scale_size_area(max_size = 2)
library(DBI)
dbGetQuery(sc, "SELECT * FROM flights LIMIT 100")
# transform our data set, and then partition into 'training', 'test'
partitions <- mtcars_tbl %>%
filter(hp >= 100) %>%
mutate(cyl8 = cyl == 8) %>%
sdf_partition(training = 0.5, test = 0.5, seed = 1099)
# fit a linear model to the training dataset
partitions$training %>%
ml_linear_regression(response = "mpg", features = c("wt", "cyl"))
partitions$training %>%
ml_kmeans(centers = 3, features = c("wt", "cyl"))
mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl,
strict_version_check = FALSE)
h2o.glm(x = c("wt", "cyl"),
y = "mpg",
training_frame = mtcars_h2o,
lambda_search = TRUE)
h2o_flow(sc, strict_version_check = FALSE)
spark_disconnect(sc)
library(sparklygraphs)
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local", version = "2.1.0")
highschool_tbl <- copy_to(sc, ggraph::highschool, "highschool")
# create a table with unique vertices using dplyr
vertices_tbl <- sdf_bind_rows(
highschool_tbl %>% distinct(from) %>% transmute(id = from),
highschool_tbl %>% distinct(to) %>% transmute(id = to)
)
# create a table with <source, destination> edges
edges_tbl <- highschool_tbl %>% transmute(src = from, dst = to)
# calculate PageRank over the highschool dataset
gf_graphframe(vertices_tbl, edges_tbl) %>%
gf_pagerank(reset_prob = 0.15, max_iter = 10L, source_id = "1")
spark_apply(highschool_tbl, function(x) {
x + rgamma(1,2)
})
spark_apply(highschool_tbl, function(x) {
x
})
spark_apply(highschool_tbl, function(x) {
x$year - 1900
}, names = "year")
spark_apply(highschool_tbl, function(x) {
paste(x$year - 1900, "s", sep = "'")
}, names = "year")
spark_apply(highschool_tbl, function(x) {
x$era <- paste(x$year - 1900, "s", sep = "'")
x
}, names = x(columnames(highschool_tbl), "era"))
spark_apply(highschool_tbl, function(x) {
x$era <- paste(x$year - 1900, "s", sep = "'")
x
}, names = c(columnames(highschool_tbl), "era"))
spark_apply(highschool_tbl, function(x) {
x$era <- paste(x$year - 1900, "s", sep = "'")
x
}, names = c(columNames(highschool_tbl), "era"))
spark_apply(highschool_tbl, function(x) {
x$era <- paste(x$year - 1900, "s", sep = "'")
x
}, names = c(colnames(highschool_tbl), "era"))
spark_apply(highschool_tbl, function(x) {
x$era <- paste("Year", x$year - 1900)
x
}, names = c(colnames(highschool_tbl), "era"))
spark_apply(highschool_tbl, function(x) {
x$era <- paste("Year", x$year)
x
}, names = c(colnames(highschool_tbl), "era"))
spark_disconnect(sc)
?revealjs
?revealjs::revealjs_presentation
library(sparklyr)
library(sparklyr)                       # Load sparklyr
spark_install()                         # Install Apache Spark
sc <- spark_connect(master = "master")  # Connect to local instance
